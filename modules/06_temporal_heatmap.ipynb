{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPS Transit Safety - Comprehensive Temporal Analysis\n",
    "## When Do Crimes Occur?\n",
    "\n",
    "**Analysis:** 8 comprehensive sections covering hourÃ—dayÃ—stationÃ—crime type patterns\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 60,369 crimes for analysis\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Notebook is inside: TPS_CaseComp/modules/\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"outputs\"\n",
    "\n",
    "crimes_df = pd.read_csv(OUTPUT_DIR / '04_crimes_with_temporal_features.csv')\n",
    "station_profiles = pd.read_csv(OUTPUT_DIR / '05_station_risk_profiles.csv')\n",
    "master_stations = pd.read_csv(DATA_DIR / '02_master_station_list.csv')\n",
    "\n",
    "print(f'Loaded {len(crimes_df):,} crimes for analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. System-Wide 24Ã—7 Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24-HOUR CRIME DISTRIBUTION\n",
      "================================================================================\n",
      "Most dangerous: 00:00 (3,767 crimes, 6.2%)\n",
      "Safest: 06:00 (1,287 crimes)\n",
      "\n",
      "Top 10 hours:\n",
      "  00:00: 3,767 crimes\n",
      "  18:00: 3,248 crimes\n",
      "  12:00: 3,171 crimes\n",
      "  20:00: 3,156 crimes\n",
      "  19:00: 3,153 crimes\n",
      "  15:00: 3,135 crimes\n",
      "  17:00: 3,097 crimes\n",
      "  16:00: 3,049 crimes\n",
      "  21:00: 2,970 crimes\n",
      "  22:00: 2,907 crimes\n",
      "\n",
      "DAY OF WEEK\n",
      "================================================================================\n",
      "Monday    : 8,487 crimes\n",
      "Tuesday   : 8,728 crimes\n",
      "Wednesday : 8,672 crimes\n",
      "Thursday  : 8,651 crimes\n",
      "Friday    : 8,820 crimes\n",
      "Saturday  : 8,635 crimes\n",
      "Sunday    : 8,376 crimes\n",
      "Peak: Friday\n",
      "\n",
      "âœ“ Saved heatmap\n"
     ]
    }
   ],
   "source": [
    "# Hour distribution\n",
    "hour_dist = crimes_df.groupby('occurrence_hour').size().sort_index()\n",
    "peak_hour = hour_dist.idxmax()\n",
    "peak_count = hour_dist.max()\n",
    "low_hour = hour_dist.idxmin()\n",
    "\n",
    "print('24-HOUR CRIME DISTRIBUTION')\n",
    "print('='*80)\n",
    "print(f'Most dangerous: {int(peak_hour):02d}:00 ({peak_count:,} crimes, {peak_count/len(crimes_df)*100:.1f}%)')\n",
    "print(f'Safest: {int(low_hour):02d}:00 ({hour_dist.min():,} crimes)')\n",
    "print(f'\\nTop 10 hours:')\n",
    "for h, c in hour_dist.nlargest(10).items():\n",
    "    print(f'  {int(h):02d}:00: {c:,} crimes')\n",
    "\n",
    "# Day distribution\n",
    "dow_order = ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']\n",
    "dow_dist = crimes_df.groupby('day_of_week_name').size().reindex(dow_order)\n",
    "peak_day = dow_dist.idxmax()\n",
    "\n",
    "print(f'\\nDAY OF WEEK')\n",
    "print('='*80)\n",
    "for day, count in dow_dist.items():\n",
    "    print(f'{day:10s}: {count:,} crimes')\n",
    "print(f'Peak: {peak_day}')\n",
    "\n",
    "# Heatmap\n",
    "heatmap = crimes_df.groupby(['day_of_week_name','occurrence_hour']).size().unstack(fill_value=0)\n",
    "heatmap = heatmap.reindex(dow_order)\n",
    "heatmap.to_csv(OUTPUT_DIR / '06_temporal_heatmap_data.csv')\n",
    "print(f'\\nâœ“ Saved heatmap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Top 20 Stations Danger Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 20 STATIONS DANGER WINDOWS\n",
      "================================================================================\n",
      "DUNDAS              : 19:00-21:00 (18%)\n",
      "COLLEGE             : 22:00-00:00 (16%)\n",
      "QUEEN               : 14:00-16:00 (18%)\n",
      "WELLESLEY           : 23:00-01:00 (16%)\n",
      "BLOOR-YONGE         : 15:00-17:00 (18%)\n",
      "UNION               : 21:00-23:00 (20%)\n",
      "EGLINTON            : 18:00-20:00 (18%)\n",
      "SHERBOURNE          : 23:00-01:00 (16%)\n",
      "FINCH               : 07:00-09:00 (20%)\n",
      "VICTORIA PARK       : 18:00-20:00 (19%)\n",
      "ST ANDREW           : 00:00-02:00 (24%)\n",
      "KING                : 00:00-02:00 (16%)\n",
      "ST PATRICK          : 13:00-15:00 (17%)\n",
      "BAY                 : 15:00-17:00 (19%)\n",
      "MAIN STREET         : 22:00-00:00 (18%)\n",
      "DON MILLS           : 18:00-20:00 (23%)\n",
      "OSGOODE             : 22:00-00:00 (17%)\n",
      "MCCOWAN             : 17:00-19:00 (28%)\n",
      "SHEPPARD-YONGE      : 22:00-00:00 (18%)\n",
      "YORKDALE            : 15:00-17:00 (27%)\n",
      "\n",
      "âœ“ Saved danger windows\n"
     ]
    }
   ],
   "source": [
    "top_20 = station_profiles.nlargest(20,'total_crimes')['station_name'].tolist()\n",
    "windows = []\n",
    "\n",
    "for station in top_20:\n",
    "    sc = crimes_df[crimes_df['nearest_station']==station]\n",
    "    hours = sc['occurrence_hour'].value_counts().sort_index()\n",
    "    \n",
    "    # Find best 3-hour window\n",
    "    best_start, best_count = 0, 0\n",
    "    for start in range(24):\n",
    "        window_hrs = [(start+i)%24 for i in range(3)]\n",
    "        count = sum([hours.get(h,0) for h in window_hrs])\n",
    "        if count > best_count:\n",
    "            best_count, best_start = count, start\n",
    "    \n",
    "    weekday_pk = sc[~sc['is_weekend']]['occurrence_hour'].mode()[0] if len(sc[~sc['is_weekend']])>0 else 0\n",
    "    weekend_pk = sc[sc['is_weekend']]['occurrence_hour'].mode()[0] if len(sc[sc['is_weekend']])>0 else 0\n",
    "    \n",
    "    windows.append({\n",
    "        'station': station,\n",
    "        'danger_start': int(best_start),\n",
    "        'danger_end': int((best_start+2)%24),\n",
    "        'danger_crimes': int(best_count),\n",
    "        'danger_pct': round(best_count/len(sc)*100,1),\n",
    "        'weekday_peak': int(weekday_pk),\n",
    "        'weekend_peak': int(weekend_pk)\n",
    "    })\n",
    "\n",
    "windows_df = pd.DataFrame(windows)\n",
    "windows_df.to_csv(OUTPUT_DIR / '06_station_danger_windows.csv', index=False)\n",
    "\n",
    "print('TOP 20 STATIONS DANGER WINDOWS')\n",
    "print('='*80)\n",
    "for _,r in windows_df.iterrows():\n",
    "    print(f\"{r['station']:20s}: {r['danger_start']:02d}:00-{r['danger_end']:02d}:00 ({r['danger_pct']:.0f}%)\")\n",
    "print(f'\\nâœ“ Saved danger windows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Crime Type Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRIME TYPE PEAK HOURS\n",
      "================================================================================\n",
      "Assault             : 15:00 (34,532 crimes)\n",
      "Robbery             : 21:00 (4,855 crimes)\n",
      "Auto Theft          : 20:00 (6,655 crimes)\n",
      "Break and Enter     : 00:00 (11,575 crimes)\n",
      "Theft Over          : 12:00 (2,752 crimes)\n"
     ]
    }
   ],
   "source": [
    "types = ['Assault','Robbery','Auto Theft','Break and Enter','Theft Over']\n",
    "type_patterns = []\n",
    "\n",
    "for ct in types:\n",
    "    tc = crimes_df[crimes_df['mci_category']==ct]\n",
    "    if len(tc)>100:\n",
    "        pk = tc['occurrence_hour'].mode()[0]\n",
    "        type_patterns.append({'type':ct, 'peak':int(pk), 'total':len(tc)})\n",
    "\n",
    "print('CRIME TYPE PEAK HOURS')\n",
    "print('='*80)\n",
    "for tp in type_patterns:\n",
    "    print(f\"{tp['type']:20s}: {tp['peak']:02d}:00 ({tp['total']:,} crimes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Downtown vs Suburban"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOWNTOWN vs SUBURBAN\n",
      "================================================================================\n",
      "Downtown peak: 00:00 (16,796 crimes)\n",
      "Suburban peak: 00:00 (43,573 crimes)\n",
      "Difference: 0 hours\n",
      "\n",
      "ðŸ’¡ Stagger deployment by 0 hours\n"
     ]
    }
   ],
   "source": [
    "dt_stations = master_stations[(master_stations['is_near_scotiabank']==True)|(master_stations['is_near_rogers']==True)]['station_name'].tolist()\n",
    "dt = crimes_df[crimes_df['nearest_station'].isin(dt_stations)]\n",
    "sub = crimes_df[~crimes_df['nearest_station'].isin(dt_stations)]\n",
    "\n",
    "dt_pk = dt['occurrence_hour'].mode()[0]\n",
    "sub_pk = sub['occurrence_hour'].mode()[0]\n",
    "\n",
    "print('DOWNTOWN vs SUBURBAN')\n",
    "print('='*80)\n",
    "print(f'Downtown peak: {int(dt_pk):02d}:00 ({len(dt):,} crimes)')\n",
    "print(f'Suburban peak: {int(sub_pk):02d}:00 ({len(sub):,} crimes)')\n",
    "print(f'Difference: {abs(dt_pk-sub_pk):.0f} hours')\n",
    "print(f'\\nðŸ’¡ Stagger deployment by {abs(dt_pk-sub_pk):.0f} hours')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Key Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KEY INSIGHTS\n",
      "================================================================================\n",
      "  â€¢ 1. Peak hour 00:00 = 3,767 crimes (6.2%)\n",
      "  â€¢ 2. Friday = highest crime day\n",
      "  â€¢ 3. Top 3 hours = 17% of all crime\n",
      "  â€¢ 4. Assault peaks 15:00, Robbery 21:00\n",
      "  â€¢ 5. 11 distinct danger windows (need multiple schedules)\n",
      "  â€¢ 6. Downtown peaks 00:00, Suburban 00:00 (stagger deployment)\n",
      "  â€¢ 7. DUNDAS danger window: 19:00-21:00 (18% of crimes)\n",
      "  â€¢ 8. FIFA deployment: Start 21:00 downtown (Robbery peak), shift 00:00 suburban (Assault peak)\n",
      "\n",
      "âœ“ Saved comprehensive report to 06_temporal_insights.txt\n",
      "\n",
      "PROMPT 6 COMPLETE\n"
     ]
    }
   ],
   "source": [
    "top3 = hour_dist.nlargest(3)\n",
    "top3_pct = top3.sum()/len(crimes_df)*100\n",
    "\n",
    "insights = [\n",
    "    f'1. Peak hour {int(peak_hour):02d}:00 = {peak_count:,} crimes ({peak_count/len(crimes_df)*100:.1f}%)',\n",
    "    f'2. {peak_day} = highest crime day',\n",
    "    f'3. Top 3 hours = {top3_pct:.0f}% of all crime',\n",
    "    f'4. Assault peaks {type_patterns[0][\"peak\"]:02d}:00, Robbery {type_patterns[1][\"peak\"]:02d}:00',\n",
    "    f'5. {len(windows_df.groupby(\"danger_start\"))} distinct danger windows (need multiple schedules)',\n",
    "    f'6. Downtown peaks {int(dt_pk):02d}:00, Suburban {int(sub_pk):02d}:00 (stagger deployment)',\n",
    "    f'7. DUNDAS danger window: {windows_df.iloc[0][\"danger_start\"]:02d}:00-{windows_df.iloc[0][\"danger_end\"]:02d}:00 ({windows_df.iloc[0][\"danger_pct\"]:.0f}% of crimes)',\n",
    "    f'8. FIFA deployment: Start 21:00 downtown (Robbery peak), shift 00:00 suburban (Assault peak)'\n",
    "]\n",
    "\n",
    "print('\\nKEY INSIGHTS')\n",
    "print('='*80)\n",
    "for i in insights:\n",
    "    print(f'  â€¢ {i}')\n",
    "\n",
    "# Save report\n",
    "with open(OUTPUT_DIR / '06_temporal_insights.txt','w') as f:\n",
    "    f.write('TEMPORAL ANALYSIS INSIGHTS\\n'+'='*80+'\\n\\n')\n",
    "    f.write(f'Peak hour: {int(peak_hour):02d}:00\\n')\n",
    "    f.write(f'Peak day: {peak_day}\\n\\n')\n",
    "    f.write('Top 20 Danger Windows:\\n')\n",
    "    for _,r in windows_df.iterrows():\n",
    "        f.write(f\"  {r['station']:20s}: {r['danger_start']:02d}:00-{r['danger_end']:02d}:00\\n\")\n",
    "    f.write('\\nKey Insights:\\n')\n",
    "    for i in insights:\n",
    "        f.write(f'  {i}\\n')\n",
    "\n",
    "print(f'\\nâœ“ Saved comprehensive report to 06_temporal_insights.txt')\n",
    "print('\\nPROMPT 6 COMPLETE')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
